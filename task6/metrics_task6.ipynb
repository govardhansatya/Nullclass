{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d87f6be0",
   "metadata": {},
   "source": [
    "# Task 6 Metrics & Validation\n",
    "Evaluate LoRA fine-tuned diffusion model with FID (optional small sample) and CLIP similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e19058eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install -q torch-fidelity open-clip-torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88fa4cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, json, numpy as np\n",
    "from pathlib import Path\n",
    "from torchvision.utils import save_image\n",
    "from torch_fidelity import calculate_metrics\n",
    "DEVICE='cuda' if torch.cuda.is_available() else 'cpu'\n",
    "LORA_DIR=Path('task6_sd_lora/unet_lora')\n",
    "assert LORA_DIR.exists(), 'Run task6_diffusion_finetune.ipynb first.'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a15a7e",
   "metadata": {},
   "source": [
    "## 1. Load Base + Apply LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d2f938a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\acm\\models\\jul25\\nullclass\\.venv\\Lib\\site-packages\\diffusers\\models\\transformers\\transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.\n",
      "  deprecate(\"Transformer2DModelOutput\", \"1.0.0\", deprecation_message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dccf3b4f101a465d9da74a7e668ac0ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "PeftModel(\n",
       "  (base_model): LoraModel(\n",
       "    (model): UNet2DConditionModel(\n",
       "      (conv_in): Conv2d(4, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (time_proj): Timesteps()\n",
       "      (time_embedding): TimestepEmbedding(\n",
       "        (linear_1): Linear(in_features=320, out_features=1280, bias=True)\n",
       "        (act): SiLU()\n",
       "        (linear_2): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      )\n",
       "      (down_blocks): ModuleList(\n",
       "        (0): CrossAttnDownBlock2D(\n",
       "          (attentions): ModuleList(\n",
       "            (0-1): 2 x Transformer2DModel(\n",
       "              (norm): GroupNorm(32, 320, eps=1e-06, affine=True)\n",
       "              (proj_in): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (transformer_blocks): ModuleList(\n",
       "                (0): BasicTransformerBlock(\n",
       "                  (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "                  (attn1): Attention(\n",
       "                    (to_q): lora.Linear(\n",
       "                      (base_layer): Linear(in_features=320, out_features=320, bias=False)\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Dropout(p=0.05, inplace=False)\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=320, out_features=4, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=4, out_features=320, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                      (lora_magnitude_vector): ModuleDict()\n",
       "                    )\n",
       "                    (to_k): lora.Linear(\n",
       "                      (base_layer): Linear(in_features=320, out_features=320, bias=False)\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Dropout(p=0.05, inplace=False)\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=320, out_features=4, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=4, out_features=320, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                      (lora_magnitude_vector): ModuleDict()\n",
       "                    )\n",
       "                    (to_v): lora.Linear(\n",
       "                      (base_layer): Linear(in_features=320, out_features=320, bias=False)\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Dropout(p=0.05, inplace=False)\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=320, out_features=4, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=4, out_features=320, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                      (lora_magnitude_vector): ModuleDict()\n",
       "                    )\n",
       "                    (to_out): ModuleList(\n",
       "                      (0): lora.Linear(\n",
       "                        (base_layer): Linear(in_features=320, out_features=320, bias=True)\n",
       "                        (lora_dropout): ModuleDict(\n",
       "                          (default): Dropout(p=0.05, inplace=False)\n",
       "                        )\n",
       "                        (lora_A): ModuleDict(\n",
       "                          (default): Linear(in_features=320, out_features=4, bias=False)\n",
       "                        )\n",
       "                        (lora_B): ModuleDict(\n",
       "                          (default): Linear(in_features=4, out_features=320, bias=False)\n",
       "                        )\n",
       "                        (lora_embedding_A): ParameterDict()\n",
       "                        (lora_embedding_B): ParameterDict()\n",
       "                        (lora_magnitude_vector): ModuleDict()\n",
       "                      )\n",
       "                      (1): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                  (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "                  (attn2): Attention(\n",
       "                    (to_q): lora.Linear(\n",
       "                      (base_layer): Linear(in_features=320, out_features=320, bias=False)\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Dropout(p=0.05, inplace=False)\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=320, out_features=4, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=4, out_features=320, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                      (lora_magnitude_vector): ModuleDict()\n",
       "                    )\n",
       "                    (to_k): lora.Linear(\n",
       "                      (base_layer): Linear(in_features=768, out_features=320, bias=False)\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Dropout(p=0.05, inplace=False)\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=768, out_features=4, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=4, out_features=320, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                      (lora_magnitude_vector): ModuleDict()\n",
       "                    )\n",
       "                    (to_v): lora.Linear(\n",
       "                      (base_layer): Linear(in_features=768, out_features=320, bias=False)\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Dropout(p=0.05, inplace=False)\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=768, out_features=4, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=4, out_features=320, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                      (lora_magnitude_vector): ModuleDict()\n",
       "                    )\n",
       "                    (to_out): ModuleList(\n",
       "                      (0): lora.Linear(\n",
       "                        (base_layer): Linear(in_features=320, out_features=320, bias=True)\n",
       "                        (lora_dropout): ModuleDict(\n",
       "                          (default): Dropout(p=0.05, inplace=False)\n",
       "                        )\n",
       "                        (lora_A): ModuleDict(\n",
       "                          (default): Linear(in_features=320, out_features=4, bias=False)\n",
       "                        )\n",
       "                        (lora_B): ModuleDict(\n",
       "                          (default): Linear(in_features=4, out_features=320, bias=False)\n",
       "                        )\n",
       "                        (lora_embedding_A): ParameterDict()\n",
       "                        (lora_embedding_B): ParameterDict()\n",
       "                        (lora_magnitude_vector): ModuleDict()\n",
       "                      )\n",
       "                      (1): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                  (norm3): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "                  (ff): FeedForward(\n",
       "                    (net): ModuleList(\n",
       "                      (0): GEGLU(\n",
       "                        (proj): Linear(in_features=320, out_features=2560, bias=True)\n",
       "                      )\n",
       "                      (1): Dropout(p=0.0, inplace=False)\n",
       "                      (2): Linear(in_features=1280, out_features=320, bias=True)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (proj_out): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "            )\n",
       "          )\n",
       "          (resnets): ModuleList(\n",
       "            (0-1): 2 x ResnetBlock2D(\n",
       "              (norm1): GroupNorm(32, 320, eps=1e-05, affine=True)\n",
       "              (conv1): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (time_emb_proj): Linear(in_features=1280, out_features=320, bias=True)\n",
       "              (norm2): GroupNorm(32, 320, eps=1e-05, affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (conv2): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (nonlinearity): SiLU()\n",
       "            )\n",
       "          )\n",
       "          (downsamplers): ModuleList(\n",
       "            (0): Downsample2D(\n",
       "              (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): CrossAttnDownBlock2D(\n",
       "          (attentions): ModuleList(\n",
       "            (0-1): 2 x Transformer2DModel(\n",
       "              (norm): GroupNorm(32, 640, eps=1e-06, affine=True)\n",
       "              (proj_in): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (transformer_blocks): ModuleList(\n",
       "                (0): BasicTransformerBlock(\n",
       "                  (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "                  (attn1): Attention(\n",
       "                    (to_q): lora.Linear(\n",
       "                      (base_layer): Linear(in_features=640, out_features=640, bias=False)\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Dropout(p=0.05, inplace=False)\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=640, out_features=4, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=4, out_features=640, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                      (lora_magnitude_vector): ModuleDict()\n",
       "                    )\n",
       "                    (to_k): lora.Linear(\n",
       "                      (base_layer): Linear(in_features=640, out_features=640, bias=False)\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Dropout(p=0.05, inplace=False)\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=640, out_features=4, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=4, out_features=640, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                      (lora_magnitude_vector): ModuleDict()\n",
       "                    )\n",
       "                    (to_v): lora.Linear(\n",
       "                      (base_layer): Linear(in_features=640, out_features=640, bias=False)\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Dropout(p=0.05, inplace=False)\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=640, out_features=4, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=4, out_features=640, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                      (lora_magnitude_vector): ModuleDict()\n",
       "                    )\n",
       "                    (to_out): ModuleList(\n",
       "                      (0): lora.Linear(\n",
       "                        (base_layer): Linear(in_features=640, out_features=640, bias=True)\n",
       "                        (lora_dropout): ModuleDict(\n",
       "                          (default): Dropout(p=0.05, inplace=False)\n",
       "                        )\n",
       "                        (lora_A): ModuleDict(\n",
       "                          (default): Linear(in_features=640, out_features=4, bias=False)\n",
       "                        )\n",
       "                        (lora_B): ModuleDict(\n",
       "                          (default): Linear(in_features=4, out_features=640, bias=False)\n",
       "                        )\n",
       "                        (lora_embedding_A): ParameterDict()\n",
       "                        (lora_embedding_B): ParameterDict()\n",
       "                        (lora_magnitude_vector): ModuleDict()\n",
       "                      )\n",
       "                      (1): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                  (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "                  (attn2): Attention(\n",
       "                    (to_q): lora.Linear(\n",
       "                      (base_layer): Linear(in_features=640, out_features=640, bias=False)\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Dropout(p=0.05, inplace=False)\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=640, out_features=4, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=4, out_features=640, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                      (lora_magnitude_vector): ModuleDict()\n",
       "                    )\n",
       "                    (to_k): lora.Linear(\n",
       "                      (base_layer): Linear(in_features=768, out_features=640, bias=False)\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Dropout(p=0.05, inplace=False)\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=768, out_features=4, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=4, out_features=640, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                      (lora_magnitude_vector): ModuleDict()\n",
       "                    )\n",
       "                    (to_v): lora.Linear(\n",
       "                      (base_layer): Linear(in_features=768, out_features=640, bias=False)\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Dropout(p=0.05, inplace=False)\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=768, out_features=4, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=4, out_features=640, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                      (lora_magnitude_vector): ModuleDict()\n",
       "                    )\n",
       "                    (to_out): ModuleList(\n",
       "                      (0): lora.Linear(\n",
       "                        (base_layer): Linear(in_features=640, out_features=640, bias=True)\n",
       "                        (lora_dropout): ModuleDict(\n",
       "                          (default): Dropout(p=0.05, inplace=False)\n",
       "                        )\n",
       "                        (lora_A): ModuleDict(\n",
       "                          (default): Linear(in_features=640, out_features=4, bias=False)\n",
       "                        )\n",
       "                        (lora_B): ModuleDict(\n",
       "                          (default): Linear(in_features=4, out_features=640, bias=False)\n",
       "                        )\n",
       "                        (lora_embedding_A): ParameterDict()\n",
       "                        (lora_embedding_B): ParameterDict()\n",
       "                        (lora_magnitude_vector): ModuleDict()\n",
       "                      )\n",
       "                      (1): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                  (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "                  (ff): FeedForward(\n",
       "                    (net): ModuleList(\n",
       "                      (0): GEGLU(\n",
       "                        (proj): Linear(in_features=640, out_features=5120, bias=True)\n",
       "                      )\n",
       "                      (1): Dropout(p=0.0, inplace=False)\n",
       "                      (2): Linear(in_features=2560, out_features=640, bias=True)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (proj_out): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "            )\n",
       "          )\n",
       "          (resnets): ModuleList(\n",
       "            (0): ResnetBlock2D(\n",
       "              (norm1): GroupNorm(32, 320, eps=1e-05, affine=True)\n",
       "              (conv1): Conv2d(320, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (time_emb_proj): Linear(in_features=1280, out_features=640, bias=True)\n",
       "              (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (conv2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (nonlinearity): SiLU()\n",
       "              (conv_shortcut): Conv2d(320, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "            )\n",
       "            (1): ResnetBlock2D(\n",
       "              (norm1): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
       "              (conv1): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (time_emb_proj): Linear(in_features=1280, out_features=640, bias=True)\n",
       "              (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (conv2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (nonlinearity): SiLU()\n",
       "            )\n",
       "          )\n",
       "          (downsamplers): ModuleList(\n",
       "            (0): Downsample2D(\n",
       "              (conv): Conv2d(640, 640, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (2): CrossAttnDownBlock2D(\n",
       "          (attentions): ModuleList(\n",
       "            (0-1): 2 x Transformer2DModel(\n",
       "              (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
       "              (proj_in): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (transformer_blocks): ModuleList(\n",
       "                (0): BasicTransformerBlock(\n",
       "                  (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "                  (attn1): Attention(\n",
       "                    (to_q): lora.Linear(\n",
       "                      (base_layer): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Dropout(p=0.05, inplace=False)\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=1280, out_features=4, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=4, out_features=1280, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                      (lora_magnitude_vector): ModuleDict()\n",
       "                    )\n",
       "                    (to_k): lora.Linear(\n",
       "                      (base_layer): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Dropout(p=0.05, inplace=False)\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=1280, out_features=4, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=4, out_features=1280, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                      (lora_magnitude_vector): ModuleDict()\n",
       "                    )\n",
       "                    (to_v): lora.Linear(\n",
       "                      (base_layer): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Dropout(p=0.05, inplace=False)\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=1280, out_features=4, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=4, out_features=1280, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                      (lora_magnitude_vector): ModuleDict()\n",
       "                    )\n",
       "                    (to_out): ModuleList(\n",
       "                      (0): lora.Linear(\n",
       "                        (base_layer): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                        (lora_dropout): ModuleDict(\n",
       "                          (default): Dropout(p=0.05, inplace=False)\n",
       "                        )\n",
       "                        (lora_A): ModuleDict(\n",
       "                          (default): Linear(in_features=1280, out_features=4, bias=False)\n",
       "                        )\n",
       "                        (lora_B): ModuleDict(\n",
       "                          (default): Linear(in_features=4, out_features=1280, bias=False)\n",
       "                        )\n",
       "                        (lora_embedding_A): ParameterDict()\n",
       "                        (lora_embedding_B): ParameterDict()\n",
       "                        (lora_magnitude_vector): ModuleDict()\n",
       "                      )\n",
       "                      (1): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                  (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "                  (attn2): Attention(\n",
       "                    (to_q): lora.Linear(\n",
       "                      (base_layer): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Dropout(p=0.05, inplace=False)\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=1280, out_features=4, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=4, out_features=1280, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                      (lora_magnitude_vector): ModuleDict()\n",
       "                    )\n",
       "                    (to_k): lora.Linear(\n",
       "                      (base_layer): Linear(in_features=768, out_features=1280, bias=False)\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Dropout(p=0.05, inplace=False)\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=768, out_features=4, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=4, out_features=1280, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                      (lora_magnitude_vector): ModuleDict()\n",
       "                    )\n",
       "                    (to_v): lora.Linear(\n",
       "                      (base_layer): Linear(in_features=768, out_features=1280, bias=False)\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Dropout(p=0.05, inplace=False)\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=768, out_features=4, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=4, out_features=1280, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                      (lora_magnitude_vector): ModuleDict()\n",
       "                    )\n",
       "                    (to_out): ModuleList(\n",
       "                      (0): lora.Linear(\n",
       "                        (base_layer): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                        (lora_dropout): ModuleDict(\n",
       "                          (default): Dropout(p=0.05, inplace=False)\n",
       "                        )\n",
       "                        (lora_A): ModuleDict(\n",
       "                          (default): Linear(in_features=1280, out_features=4, bias=False)\n",
       "                        )\n",
       "                        (lora_B): ModuleDict(\n",
       "                          (default): Linear(in_features=4, out_features=1280, bias=False)\n",
       "                        )\n",
       "                        (lora_embedding_A): ParameterDict()\n",
       "                        (lora_embedding_B): ParameterDict()\n",
       "                        (lora_magnitude_vector): ModuleDict()\n",
       "                      )\n",
       "                      (1): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                  (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "                  (ff): FeedForward(\n",
       "                    (net): ModuleList(\n",
       "                      (0): GEGLU(\n",
       "                        (proj): Linear(in_features=1280, out_features=10240, bias=True)\n",
       "                      )\n",
       "                      (1): Dropout(p=0.0, inplace=False)\n",
       "                      (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (proj_out): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "            )\n",
       "          )\n",
       "          (resnets): ModuleList(\n",
       "            (0): ResnetBlock2D(\n",
       "              (norm1): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
       "              (conv1): Conv2d(640, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "              (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (conv2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (nonlinearity): SiLU()\n",
       "              (conv_shortcut): Conv2d(640, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "            )\n",
       "            (1): ResnetBlock2D(\n",
       "              (norm1): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "              (conv1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "              (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (conv2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (nonlinearity): SiLU()\n",
       "            )\n",
       "          )\n",
       "          (downsamplers): ModuleList(\n",
       "            (0): Downsample2D(\n",
       "              (conv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (3): DownBlock2D(\n",
       "          (resnets): ModuleList(\n",
       "            (0-1): 2 x ResnetBlock2D(\n",
       "              (norm1): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "              (conv1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "              (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (conv2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (nonlinearity): SiLU()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (up_blocks): ModuleList(\n",
       "        (0): UpBlock2D(\n",
       "          (resnets): ModuleList(\n",
       "            (0-2): 3 x ResnetBlock2D(\n",
       "              (norm1): GroupNorm(32, 2560, eps=1e-05, affine=True)\n",
       "              (conv1): Conv2d(2560, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "              (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (conv2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (nonlinearity): SiLU()\n",
       "              (conv_shortcut): Conv2d(2560, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "            )\n",
       "          )\n",
       "          (upsamplers): ModuleList(\n",
       "            (0): Upsample2D(\n",
       "              (conv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): CrossAttnUpBlock2D(\n",
       "          (attentions): ModuleList(\n",
       "            (0-2): 3 x Transformer2DModel(\n",
       "              (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
       "              (proj_in): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (transformer_blocks): ModuleList(\n",
       "                (0): BasicTransformerBlock(\n",
       "                  (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "                  (attn1): Attention(\n",
       "                    (to_q): lora.Linear(\n",
       "                      (base_layer): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Dropout(p=0.05, inplace=False)\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=1280, out_features=4, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=4, out_features=1280, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                      (lora_magnitude_vector): ModuleDict()\n",
       "                    )\n",
       "                    (to_k): lora.Linear(\n",
       "                      (base_layer): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Dropout(p=0.05, inplace=False)\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=1280, out_features=4, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=4, out_features=1280, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                      (lora_magnitude_vector): ModuleDict()\n",
       "                    )\n",
       "                    (to_v): lora.Linear(\n",
       "                      (base_layer): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Dropout(p=0.05, inplace=False)\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=1280, out_features=4, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=4, out_features=1280, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                      (lora_magnitude_vector): ModuleDict()\n",
       "                    )\n",
       "                    (to_out): ModuleList(\n",
       "                      (0): lora.Linear(\n",
       "                        (base_layer): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                        (lora_dropout): ModuleDict(\n",
       "                          (default): Dropout(p=0.05, inplace=False)\n",
       "                        )\n",
       "                        (lora_A): ModuleDict(\n",
       "                          (default): Linear(in_features=1280, out_features=4, bias=False)\n",
       "                        )\n",
       "                        (lora_B): ModuleDict(\n",
       "                          (default): Linear(in_features=4, out_features=1280, bias=False)\n",
       "                        )\n",
       "                        (lora_embedding_A): ParameterDict()\n",
       "                        (lora_embedding_B): ParameterDict()\n",
       "                        (lora_magnitude_vector): ModuleDict()\n",
       "                      )\n",
       "                      (1): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                  (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "                  (attn2): Attention(\n",
       "                    (to_q): lora.Linear(\n",
       "                      (base_layer): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Dropout(p=0.05, inplace=False)\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=1280, out_features=4, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=4, out_features=1280, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                      (lora_magnitude_vector): ModuleDict()\n",
       "                    )\n",
       "                    (to_k): lora.Linear(\n",
       "                      (base_layer): Linear(in_features=768, out_features=1280, bias=False)\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Dropout(p=0.05, inplace=False)\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=768, out_features=4, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=4, out_features=1280, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                      (lora_magnitude_vector): ModuleDict()\n",
       "                    )\n",
       "                    (to_v): lora.Linear(\n",
       "                      (base_layer): Linear(in_features=768, out_features=1280, bias=False)\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Dropout(p=0.05, inplace=False)\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=768, out_features=4, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=4, out_features=1280, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                      (lora_magnitude_vector): ModuleDict()\n",
       "                    )\n",
       "                    (to_out): ModuleList(\n",
       "                      (0): lora.Linear(\n",
       "                        (base_layer): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                        (lora_dropout): ModuleDict(\n",
       "                          (default): Dropout(p=0.05, inplace=False)\n",
       "                        )\n",
       "                        (lora_A): ModuleDict(\n",
       "                          (default): Linear(in_features=1280, out_features=4, bias=False)\n",
       "                        )\n",
       "                        (lora_B): ModuleDict(\n",
       "                          (default): Linear(in_features=4, out_features=1280, bias=False)\n",
       "                        )\n",
       "                        (lora_embedding_A): ParameterDict()\n",
       "                        (lora_embedding_B): ParameterDict()\n",
       "                        (lora_magnitude_vector): ModuleDict()\n",
       "                      )\n",
       "                      (1): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                  (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "                  (ff): FeedForward(\n",
       "                    (net): ModuleList(\n",
       "                      (0): GEGLU(\n",
       "                        (proj): Linear(in_features=1280, out_features=10240, bias=True)\n",
       "                      )\n",
       "                      (1): Dropout(p=0.0, inplace=False)\n",
       "                      (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (proj_out): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "            )\n",
       "          )\n",
       "          (resnets): ModuleList(\n",
       "            (0-1): 2 x ResnetBlock2D(\n",
       "              (norm1): GroupNorm(32, 2560, eps=1e-05, affine=True)\n",
       "              (conv1): Conv2d(2560, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "              (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (conv2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (nonlinearity): SiLU()\n",
       "              (conv_shortcut): Conv2d(2560, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "            )\n",
       "            (2): ResnetBlock2D(\n",
       "              (norm1): GroupNorm(32, 1920, eps=1e-05, affine=True)\n",
       "              (conv1): Conv2d(1920, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "              (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (conv2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (nonlinearity): SiLU()\n",
       "              (conv_shortcut): Conv2d(1920, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "            )\n",
       "          )\n",
       "          (upsamplers): ModuleList(\n",
       "            (0): Upsample2D(\n",
       "              (conv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (2): CrossAttnUpBlock2D(\n",
       "          (attentions): ModuleList(\n",
       "            (0-2): 3 x Transformer2DModel(\n",
       "              (norm): GroupNorm(32, 640, eps=1e-06, affine=True)\n",
       "              (proj_in): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (transformer_blocks): ModuleList(\n",
       "                (0): BasicTransformerBlock(\n",
       "                  (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "                  (attn1): Attention(\n",
       "                    (to_q): lora.Linear(\n",
       "                      (base_layer): Linear(in_features=640, out_features=640, bias=False)\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Dropout(p=0.05, inplace=False)\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=640, out_features=4, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=4, out_features=640, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                      (lora_magnitude_vector): ModuleDict()\n",
       "                    )\n",
       "                    (to_k): lora.Linear(\n",
       "                      (base_layer): Linear(in_features=640, out_features=640, bias=False)\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Dropout(p=0.05, inplace=False)\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=640, out_features=4, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=4, out_features=640, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                      (lora_magnitude_vector): ModuleDict()\n",
       "                    )\n",
       "                    (to_v): lora.Linear(\n",
       "                      (base_layer): Linear(in_features=640, out_features=640, bias=False)\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Dropout(p=0.05, inplace=False)\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=640, out_features=4, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=4, out_features=640, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                      (lora_magnitude_vector): ModuleDict()\n",
       "                    )\n",
       "                    (to_out): ModuleList(\n",
       "                      (0): lora.Linear(\n",
       "                        (base_layer): Linear(in_features=640, out_features=640, bias=True)\n",
       "                        (lora_dropout): ModuleDict(\n",
       "                          (default): Dropout(p=0.05, inplace=False)\n",
       "                        )\n",
       "                        (lora_A): ModuleDict(\n",
       "                          (default): Linear(in_features=640, out_features=4, bias=False)\n",
       "                        )\n",
       "                        (lora_B): ModuleDict(\n",
       "                          (default): Linear(in_features=4, out_features=640, bias=False)\n",
       "                        )\n",
       "                        (lora_embedding_A): ParameterDict()\n",
       "                        (lora_embedding_B): ParameterDict()\n",
       "                        (lora_magnitude_vector): ModuleDict()\n",
       "                      )\n",
       "                      (1): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                  (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "                  (attn2): Attention(\n",
       "                    (to_q): lora.Linear(\n",
       "                      (base_layer): Linear(in_features=640, out_features=640, bias=False)\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Dropout(p=0.05, inplace=False)\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=640, out_features=4, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=4, out_features=640, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                      (lora_magnitude_vector): ModuleDict()\n",
       "                    )\n",
       "                    (to_k): lora.Linear(\n",
       "                      (base_layer): Linear(in_features=768, out_features=640, bias=False)\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Dropout(p=0.05, inplace=False)\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=768, out_features=4, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=4, out_features=640, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                      (lora_magnitude_vector): ModuleDict()\n",
       "                    )\n",
       "                    (to_v): lora.Linear(\n",
       "                      (base_layer): Linear(in_features=768, out_features=640, bias=False)\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Dropout(p=0.05, inplace=False)\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=768, out_features=4, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=4, out_features=640, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                      (lora_magnitude_vector): ModuleDict()\n",
       "                    )\n",
       "                    (to_out): ModuleList(\n",
       "                      (0): lora.Linear(\n",
       "                        (base_layer): Linear(in_features=640, out_features=640, bias=True)\n",
       "                        (lora_dropout): ModuleDict(\n",
       "                          (default): Dropout(p=0.05, inplace=False)\n",
       "                        )\n",
       "                        (lora_A): ModuleDict(\n",
       "                          (default): Linear(in_features=640, out_features=4, bias=False)\n",
       "                        )\n",
       "                        (lora_B): ModuleDict(\n",
       "                          (default): Linear(in_features=4, out_features=640, bias=False)\n",
       "                        )\n",
       "                        (lora_embedding_A): ParameterDict()\n",
       "                        (lora_embedding_B): ParameterDict()\n",
       "                        (lora_magnitude_vector): ModuleDict()\n",
       "                      )\n",
       "                      (1): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                  (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "                  (ff): FeedForward(\n",
       "                    (net): ModuleList(\n",
       "                      (0): GEGLU(\n",
       "                        (proj): Linear(in_features=640, out_features=5120, bias=True)\n",
       "                      )\n",
       "                      (1): Dropout(p=0.0, inplace=False)\n",
       "                      (2): Linear(in_features=2560, out_features=640, bias=True)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (proj_out): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "            )\n",
       "          )\n",
       "          (resnets): ModuleList(\n",
       "            (0): ResnetBlock2D(\n",
       "              (norm1): GroupNorm(32, 1920, eps=1e-05, affine=True)\n",
       "              (conv1): Conv2d(1920, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (time_emb_proj): Linear(in_features=1280, out_features=640, bias=True)\n",
       "              (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (conv2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (nonlinearity): SiLU()\n",
       "              (conv_shortcut): Conv2d(1920, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "            )\n",
       "            (1): ResnetBlock2D(\n",
       "              (norm1): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "              (conv1): Conv2d(1280, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (time_emb_proj): Linear(in_features=1280, out_features=640, bias=True)\n",
       "              (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (conv2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (nonlinearity): SiLU()\n",
       "              (conv_shortcut): Conv2d(1280, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "            )\n",
       "            (2): ResnetBlock2D(\n",
       "              (norm1): GroupNorm(32, 960, eps=1e-05, affine=True)\n",
       "              (conv1): Conv2d(960, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (time_emb_proj): Linear(in_features=1280, out_features=640, bias=True)\n",
       "              (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (conv2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (nonlinearity): SiLU()\n",
       "              (conv_shortcut): Conv2d(960, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "            )\n",
       "          )\n",
       "          (upsamplers): ModuleList(\n",
       "            (0): Upsample2D(\n",
       "              (conv): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (3): CrossAttnUpBlock2D(\n",
       "          (attentions): ModuleList(\n",
       "            (0-2): 3 x Transformer2DModel(\n",
       "              (norm): GroupNorm(32, 320, eps=1e-06, affine=True)\n",
       "              (proj_in): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (transformer_blocks): ModuleList(\n",
       "                (0): BasicTransformerBlock(\n",
       "                  (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "                  (attn1): Attention(\n",
       "                    (to_q): lora.Linear(\n",
       "                      (base_layer): Linear(in_features=320, out_features=320, bias=False)\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Dropout(p=0.05, inplace=False)\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=320, out_features=4, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=4, out_features=320, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                      (lora_magnitude_vector): ModuleDict()\n",
       "                    )\n",
       "                    (to_k): lora.Linear(\n",
       "                      (base_layer): Linear(in_features=320, out_features=320, bias=False)\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Dropout(p=0.05, inplace=False)\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=320, out_features=4, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=4, out_features=320, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                      (lora_magnitude_vector): ModuleDict()\n",
       "                    )\n",
       "                    (to_v): lora.Linear(\n",
       "                      (base_layer): Linear(in_features=320, out_features=320, bias=False)\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Dropout(p=0.05, inplace=False)\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=320, out_features=4, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=4, out_features=320, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                      (lora_magnitude_vector): ModuleDict()\n",
       "                    )\n",
       "                    (to_out): ModuleList(\n",
       "                      (0): lora.Linear(\n",
       "                        (base_layer): Linear(in_features=320, out_features=320, bias=True)\n",
       "                        (lora_dropout): ModuleDict(\n",
       "                          (default): Dropout(p=0.05, inplace=False)\n",
       "                        )\n",
       "                        (lora_A): ModuleDict(\n",
       "                          (default): Linear(in_features=320, out_features=4, bias=False)\n",
       "                        )\n",
       "                        (lora_B): ModuleDict(\n",
       "                          (default): Linear(in_features=4, out_features=320, bias=False)\n",
       "                        )\n",
       "                        (lora_embedding_A): ParameterDict()\n",
       "                        (lora_embedding_B): ParameterDict()\n",
       "                        (lora_magnitude_vector): ModuleDict()\n",
       "                      )\n",
       "                      (1): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                  (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "                  (attn2): Attention(\n",
       "                    (to_q): lora.Linear(\n",
       "                      (base_layer): Linear(in_features=320, out_features=320, bias=False)\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Dropout(p=0.05, inplace=False)\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=320, out_features=4, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=4, out_features=320, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                      (lora_magnitude_vector): ModuleDict()\n",
       "                    )\n",
       "                    (to_k): lora.Linear(\n",
       "                      (base_layer): Linear(in_features=768, out_features=320, bias=False)\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Dropout(p=0.05, inplace=False)\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=768, out_features=4, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=4, out_features=320, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                      (lora_magnitude_vector): ModuleDict()\n",
       "                    )\n",
       "                    (to_v): lora.Linear(\n",
       "                      (base_layer): Linear(in_features=768, out_features=320, bias=False)\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Dropout(p=0.05, inplace=False)\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=768, out_features=4, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=4, out_features=320, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                      (lora_magnitude_vector): ModuleDict()\n",
       "                    )\n",
       "                    (to_out): ModuleList(\n",
       "                      (0): lora.Linear(\n",
       "                        (base_layer): Linear(in_features=320, out_features=320, bias=True)\n",
       "                        (lora_dropout): ModuleDict(\n",
       "                          (default): Dropout(p=0.05, inplace=False)\n",
       "                        )\n",
       "                        (lora_A): ModuleDict(\n",
       "                          (default): Linear(in_features=320, out_features=4, bias=False)\n",
       "                        )\n",
       "                        (lora_B): ModuleDict(\n",
       "                          (default): Linear(in_features=4, out_features=320, bias=False)\n",
       "                        )\n",
       "                        (lora_embedding_A): ParameterDict()\n",
       "                        (lora_embedding_B): ParameterDict()\n",
       "                        (lora_magnitude_vector): ModuleDict()\n",
       "                      )\n",
       "                      (1): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                  (norm3): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "                  (ff): FeedForward(\n",
       "                    (net): ModuleList(\n",
       "                      (0): GEGLU(\n",
       "                        (proj): Linear(in_features=320, out_features=2560, bias=True)\n",
       "                      )\n",
       "                      (1): Dropout(p=0.0, inplace=False)\n",
       "                      (2): Linear(in_features=1280, out_features=320, bias=True)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (proj_out): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "            )\n",
       "          )\n",
       "          (resnets): ModuleList(\n",
       "            (0): ResnetBlock2D(\n",
       "              (norm1): GroupNorm(32, 960, eps=1e-05, affine=True)\n",
       "              (conv1): Conv2d(960, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (time_emb_proj): Linear(in_features=1280, out_features=320, bias=True)\n",
       "              (norm2): GroupNorm(32, 320, eps=1e-05, affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (conv2): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (nonlinearity): SiLU()\n",
       "              (conv_shortcut): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "            )\n",
       "            (1-2): 2 x ResnetBlock2D(\n",
       "              (norm1): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
       "              (conv1): Conv2d(640, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (time_emb_proj): Linear(in_features=1280, out_features=320, bias=True)\n",
       "              (norm2): GroupNorm(32, 320, eps=1e-05, affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (conv2): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (nonlinearity): SiLU()\n",
       "              (conv_shortcut): Conv2d(640, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (mid_block): UNetMidBlock2DCrossAttn(\n",
       "        (attentions): ModuleList(\n",
       "          (0): Transformer2DModel(\n",
       "            (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
       "            (proj_in): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (transformer_blocks): ModuleList(\n",
       "              (0): BasicTransformerBlock(\n",
       "                (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "                (attn1): Attention(\n",
       "                  (to_q): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.05, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1280, out_features=4, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=4, out_features=1280, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (to_k): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.05, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1280, out_features=4, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=4, out_features=1280, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (to_v): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.05, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1280, out_features=4, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=4, out_features=1280, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (to_out): ModuleList(\n",
       "                    (0): lora.Linear(\n",
       "                      (base_layer): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Dropout(p=0.05, inplace=False)\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=1280, out_features=4, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=4, out_features=1280, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                      (lora_magnitude_vector): ModuleDict()\n",
       "                    )\n",
       "                    (1): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "                (attn2): Attention(\n",
       "                  (to_q): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.05, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1280, out_features=4, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=4, out_features=1280, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (to_k): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=768, out_features=1280, bias=False)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.05, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=4, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=4, out_features=1280, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (to_v): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=768, out_features=1280, bias=False)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.05, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=4, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=4, out_features=1280, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (to_out): ModuleList(\n",
       "                    (0): lora.Linear(\n",
       "                      (base_layer): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Dropout(p=0.05, inplace=False)\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=1280, out_features=4, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=4, out_features=1280, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                      (lora_magnitude_vector): ModuleDict()\n",
       "                    )\n",
       "                    (1): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "                (ff): FeedForward(\n",
       "                  (net): ModuleList(\n",
       "                    (0): GEGLU(\n",
       "                      (proj): Linear(in_features=1280, out_features=10240, bias=True)\n",
       "                    )\n",
       "                    (1): Dropout(p=0.0, inplace=False)\n",
       "                    (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (proj_out): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (resnets): ModuleList(\n",
       "          (0-1): 2 x ResnetBlock2D(\n",
       "            (norm1): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "            (conv1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (nonlinearity): SiLU()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (conv_norm_out): GroupNorm(32, 320, eps=1e-05, affine=True)\n",
       "      (conv_act): SiLU()\n",
       "      (conv_out): Conv2d(320, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from diffusers import StableDiffusionPipeline\n",
    "from peft import PeftModel\n",
    "pipe=StableDiffusionPipeline.from_pretrained('runwayml/stable-diffusion-v1-5', torch_dtype=torch.float16 if DEVICE=='cuda' else torch.float32).to(DEVICE)\n",
    "# Load LoRA weights from the correct directory containing adapter_config.json\n",
    "pipe.unet = PeftModel.from_pretrained(pipe.unet, str(LORA_DIR))\n",
    "pipe.unet.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334deb31",
   "metadata": {},
   "source": [
    "## 2. Generate Sample & Reference Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b7330ef8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db2a610200c54b2c901c550a4a7cf2ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72aaab96b2ca4405ab2d1692f554102c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b116d97ce8e54b468654e25d7908d97f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompts=['a cute pastel pokemon','a fiery red pokemon','a blue aquatic creature']\n",
    "gen_dir=Path('task6_gen'); ref_dir=Path('task6_ref'); gen_dir.mkdir(exist_ok=True); ref_dir.mkdir(exist_ok=True)\n",
    "# Create reference images only once\n",
    "if not any(ref_dir.iterdir()):\n",
    "    base=StableDiffusionPipeline.from_pretrained('runwayml/stable-diffusion-v1-5').to(DEVICE)\n",
    "    for i,p in enumerate(prompts):\n",
    "        img=base(p, num_inference_steps=25).images[0]\n",
    "        (ref_dir/f'{i}.png').parent.mkdir(exist_ok=True, parents=True)\n",
    "        img.save(ref_dir/f'{i}.png')\n",
    "# Generate (or reuse) LoRA images; skip ones already present to avoid FileNotFound later\n",
    "for i,p in enumerate(prompts):\n",
    "    out_path = gen_dir/f'{i}.png'\n",
    "    if not out_path.exists():\n",
    "        img=pipe(p, num_inference_steps=25).images[0]\n",
    "        out_path.parent.mkdir(exist_ok=True, parents=True)\n",
    "        img.save(out_path)\n",
    "len(list(gen_dir.iterdir()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af2e140",
   "metadata": {},
   "source": [
    "## 3. CLIP Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6076c68f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e64c539a15034783bf8cba600e082ca0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "open_clip_model.safetensors:   0%|          | 0.00/605M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error while downloading from https://cdn-lfs.hf.co/repos/f4/91/f49112076c029a4dafd8d687e7bfe82825a896cd9f38c366cf9b6b8799a86f32/e6d1bd7789aa45192b3bf90570a789b478bae1b74ebcce7eddd908e83a2b7c31?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27open_clip_model.safetensors%3B+filename%3D%22open_clip_model.safetensors%22%3B&Expires=1755425521&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc1NTQyNTUyMX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5oZi5jby9yZXBvcy9mNC85MS9mNDkxMTIwNzZjMDI5YTRkYWZkOGQ2ODdlN2JmZTgyODI1YTg5NmNkOWYzOGMzNjZjZjliNmI4Nzk5YTg2ZjMyL2U2ZDFiZDc3ODlhYTQ1MTkyYjNiZjkwNTcwYTc4OWI0NzhiYWUxYjc0ZWJjY2U3ZWRkZDkwOGU4M2EyYjdjMzE%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=QUKz2Mh8AsKXEGp2fP51CoIyIHSD25c4TKs4USaPs6WoAWtMamhtA3L9VQo5IomaEK57WQuWAZQbt1GFNpwH4G83k2FsoBtSlzroQudvKut-p1xUWHVtWHMG10EEp0YOxLGe-2uSakBpc7Dkgdp6qtXQCisQ6klVQ3RQJrldRUh9sMSFTt%7EslHeMG9YolGPQ%7EC4Qb1wRmk00ZXdbL3AmJsduHq6IvWUiiYqwZYHk1hQBLE5xyt6p1jAhWBXNuE3KhOHyScMgThzm-WpMGLywEAc9xgzS9Tj%7EX9TYgntDILt3oUy9kdg4br1X3GVdL3aQVqxAtyTs%7EWEJxtgM21tcFQ__&Key-Pair-Id=K3RPWS32NSSJCE: HTTPSConnectionPool(host='cdn-lfs.hf.co', port=443): Read timed out.\n",
      "Trying to resume download...\n",
      "WARNING:huggingface_hub.file_download:Error while downloading from https://cdn-lfs.hf.co/repos/f4/91/f49112076c029a4dafd8d687e7bfe82825a896cd9f38c366cf9b6b8799a86f32/e6d1bd7789aa45192b3bf90570a789b478bae1b74ebcce7eddd908e83a2b7c31?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27open_clip_model.safetensors%3B+filename%3D%22open_clip_model.safetensors%22%3B&Expires=1755425521&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc1NTQyNTUyMX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5oZi5jby9yZXBvcy9mNC85MS9mNDkxMTIwNzZjMDI5YTRkYWZkOGQ2ODdlN2JmZTgyODI1YTg5NmNkOWYzOGMzNjZjZjliNmI4Nzk5YTg2ZjMyL2U2ZDFiZDc3ODlhYTQ1MTkyYjNiZjkwNTcwYTc4OWI0NzhiYWUxYjc0ZWJjY2U3ZWRkZDkwOGU4M2EyYjdjMzE%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=QUKz2Mh8AsKXEGp2fP51CoIyIHSD25c4TKs4USaPs6WoAWtMamhtA3L9VQo5IomaEK57WQuWAZQbt1GFNpwH4G83k2FsoBtSlzroQudvKut-p1xUWHVtWHMG10EEp0YOxLGe-2uSakBpc7Dkgdp6qtXQCisQ6klVQ3RQJrldRUh9sMSFTt%7EslHeMG9YolGPQ%7EC4Qb1wRmk00ZXdbL3AmJsduHq6IvWUiiYqwZYHk1hQBLE5xyt6p1jAhWBXNuE3KhOHyScMgThzm-WpMGLywEAc9xgzS9Tj%7EX9TYgntDILt3oUy9kdg4br1X3GVdL3aQVqxAtyTs%7EWEJxtgM21tcFQ__&Key-Pair-Id=K3RPWS32NSSJCE: HTTPSConnectionPool(host='cdn-lfs.hf.co', port=443): Read timed out.\n",
      "Trying to resume download...\n",
      "WARNING:huggingface_hub.file_download:Error while downloading from https://cdn-lfs.hf.co/repos/f4/91/f49112076c029a4dafd8d687e7bfe82825a896cd9f38c366cf9b6b8799a86f32/e6d1bd7789aa45192b3bf90570a789b478bae1b74ebcce7eddd908e83a2b7c31?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27open_clip_model.safetensors%3B+filename%3D%22open_clip_model.safetensors%22%3B&Expires=1755425521&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc1NTQyNTUyMX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5oZi5jby9yZXBvcy9mNC85MS9mNDkxMTIwNzZjMDI5YTRkYWZkOGQ2ODdlN2JmZTgyODI1YTg5NmNkOWYzOGMzNjZjZjliNmI4Nzk5YTg2ZjMyL2U2ZDFiZDc3ODlhYTQ1MTkyYjNiZjkwNTcwYTc4OWI0NzhiYWUxYjc0ZWJjY2U3ZWRkZDkwOGU4M2EyYjdjMzE%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=QUKz2Mh8AsKXEGp2fP51CoIyIHSD25c4TKs4USaPs6WoAWtMamhtA3L9VQo5IomaEK57WQuWAZQbt1GFNpwH4G83k2FsoBtSlzroQudvKut-p1xUWHVtWHMG10EEp0YOxLGe-2uSakBpc7Dkgdp6qtXQCisQ6klVQ3RQJrldRUh9sMSFTt%7EslHeMG9YolGPQ%7EC4Qb1wRmk00ZXdbL3AmJsduHq6IvWUiiYqwZYHk1hQBLE5xyt6p1jAhWBXNuE3KhOHyScMgThzm-WpMGLywEAc9xgzS9Tj%7EX9TYgntDILt3oUy9kdg4br1X3GVdL3aQVqxAtyTs%7EWEJxtgM21tcFQ__&Key-Pair-Id=K3RPWS32NSSJCE: HTTPSConnectionPool(host='cdn-lfs.hf.co', port=443): Read timed out.\n",
      "Trying to resume download...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edeb93901baa48d4bf1cd7b29550e05b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "open_clip_model.safetensors:  94%|#########3| 566M/605M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\acm\\models\\jul25\\nullclass\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\GOVARDHAN SATYA\\.cache\\huggingface\\hub\\models--timm--vit_base_patch32_clip_224.openai. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "d:\\acm\\models\\jul25\\nullclass\\.venv\\Lib\\site-packages\\open_clip\\factory.py:450: UserWarning: QuickGELU mismatch between final model config (quick_gelu=False) and pretrained tag 'openai' (quick_gelu=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.27844351530075073, 0.3201940059661865, 0.32271507382392883]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import open_clip, PIL.Image as Image, torchvision.transforms as T\n",
    "model, _, preprocess = open_clip.create_model_and_transforms('ViT-B-32', pretrained='openai')\n",
    "tokenizer = open_clip.get_tokenizer('ViT-B-32')\n",
    "model.to(DEVICE).eval()\n",
    "clip_scores=[]\n",
    "for p in prompts:\n",
    "    img=Image.open(gen_dir/f'{prompts.index(p)}.png').convert('RGB')\n",
    "    image_input=preprocess(img).unsqueeze(0).to(DEVICE)\n",
    "    text_input=tokenizer([p]).to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        im_f, tx_f = model.encode_image(image_input), model.encode_text(text_input)\n",
    "        im_f/=im_f.norm(dim=-1, keepdim=True); tx_f/=tx_f.norm(dim=-1, keepdim=True)\n",
    "        sim=(im_f*tx_f).sum().item(); clip_scores.append(sim)\n",
    "clip_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b97aff",
   "metadata": {},
   "source": [
    "## 4. (Optional) FID (tiny set - not statistically robust)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "83e21fa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\acm\\models\\jul25\\nullclass\\.venv\\Lib\\site-packages\\torch_fidelity\\datasets.py:16: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  img = torch.ByteTensor(torch.ByteStorage.from_buffer(img.tobytes())).view(height, width, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'frechet_inception_distance': 445.96179130944034}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Explicitly control CUDA usage: torch-fidelity defaults to cuda=True which breaks on CPU-only builds\n",
    "metrics = calculate_metrics(\n",
    "\tinput1=str(gen_dir),\n",
    "\tinput2=str(ref_dir),\n",
    "\tfid=True,\n",
    "\tverbose=False,\n",
    "\tcuda=(DEVICE == 'cuda')\n",
    ")\n",
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c447213",
   "metadata": {},
   "source": [
    "## 5. Save Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f8e78dc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'clip_mean': 0.3071175316969554,\n",
       " 'clip_scores': [0.27844351530075073, 0.3201940059661865, 0.32271507382392883],\n",
       " 'fid': 445.96179130944034}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res={'clip_mean': float(np.mean(clip_scores)), 'clip_scores': clip_scores, 'fid': metrics.get('frechet_inception_distance')} \n",
    "json.dump(res, open('task6_metrics.json','w'), indent=2); res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361ac5c6",
   "metadata": {},
   "source": [
    "## 6. Summary\n",
    "Metrics saved to task6_metrics.json (note: tiny sample; enlarge for reliability)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
