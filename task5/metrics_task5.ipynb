{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11ab2a3e",
   "metadata": {},
   "source": [
    "# Task 5 Metrics & Validation\n",
    "Analyze trained attention CGAN outputs with FID (optional), CLIP score (placeholder), and conditional accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "567e93aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install -q torch-fidelity open-clip-torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "30b00586",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using task4.gans from: D:\\acm\\models\\jul25\\nullclass\\task4\\gans.py\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AttnGenerator(\n",
       "  (label_emb): Embedding(10, 50)\n",
       "  (project): Sequential(\n",
       "    (0): Linear(in_features=150, out_features=6272, bias=True)\n",
       "    (1): BatchNorm1d(6272, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "  )\n",
       "  (net): Sequential(\n",
       "    (0): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (3): SelfAttention2d(\n",
       "      (f): Conv2d(64, 8, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (g): Conv2d(64, 8, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (h): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (v): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "    (4): ConvTranspose2d(64, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (5): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (7): Conv2d(32, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch, os, numpy as np, json\n",
    "from pathlib import Path\n",
    "from torch_fidelity import calculate_metrics\n",
    "import sys, importlib, importlib.util, types, inspect\n",
    "\n",
    "# Robust path + explicit module resolution for task4.gans\n",
    "CWD = Path.cwd().resolve()\n",
    "POSSIBLE_ROOTS = [CWD, CWD.parent]\n",
    "# Insert roots (not their parents' task4) early so package discovery prefers workspace copy\n",
    "for r in POSSIBLE_ROOTS:\n",
    "    if str(r) not in sys.path:\n",
    "        sys.path.insert(0, str(r))\n",
    "\n",
    "loaded = False\n",
    "module = None\n",
    "first_error = None\n",
    "try:\n",
    "    module = importlib.import_module('task4.gans')\n",
    "except Exception as e:  # capture but keep trying explicit file load\n",
    "    first_error = e\n",
    "\n",
    "# If module imported but lacks AttnGenerator, or import failed, try explicit file targeting the version that defines class AttnGenerator\n",
    "need_explicit = module is None or not hasattr(module, 'AttnGenerator')\n",
    "if need_explicit:\n",
    "    for root in POSSIBLE_ROOTS:\n",
    "        gans_path = root / 'task4' / 'gans.py'\n",
    "        try:\n",
    "            if gans_path.exists() and 'class AttnGenerator' in gans_path.read_text(encoding='utf-8', errors='ignore'):\n",
    "                spec = importlib.util.spec_from_file_location('task4.gans', gans_path)\n",
    "                mod = importlib.util.module_from_spec(spec)\n",
    "                spec.loader.exec_module(mod)  # type: ignore\n",
    "                sys.modules['task4.gans'] = mod\n",
    "                module = mod\n",
    "                break\n",
    "        except Exception as ee:\n",
    "            first_error = first_error or ee\n",
    "\n",
    "if module is None or not hasattr(module, 'AttnGenerator'):\n",
    "    snippet = sys.path[:6]\n",
    "    raise ImportError(f\"Could not load AttnGenerator from task4.gans. First error={first_error}. sys.path head={snippet}\")\n",
    "\n",
    "# Extract required symbols\n",
    "AttnGenerator = module.AttnGenerator\n",
    "LATENT_DIM = module.LATENT_DIM\n",
    "conditional_accuracy = module.conditional_accuracy\n",
    "_build_or_load_mnist_classifier = module._build_or_load_mnist_classifier\n",
    "\n",
    "print('Using task4.gans from:', inspect.getfile(module))\n",
    "\n",
    "from torchvision.utils import save_image\n",
    "DEVICE=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "OUT='task5_outputs'\n",
    "G=AttnGenerator().to(DEVICE)\n",
    "w=Path(OUT)/'attn_generator.pt'\n",
    "assert w.exists(), 'Train Task5 model first and ensure attn_generator.pt exists in task5_outputs/'\n",
    "G.load_state_dict(torch.load(w, map_location=DEVICE)); G.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7b559b",
   "metadata": {},
   "source": [
    "## 1. Generate Sample Set for Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8b8736a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_dir=Path('task5_samples'); sample_dir.mkdir(exist_ok=True)\n",
    "with torch.no_grad():\n",
    "    for i in range(500):\n",
    "        lbl=torch.randint(0,10,(1,),device=DEVICE)\n",
    "        z=torch.randn(1, LATENT_DIM, device=DEVICE)\n",
    "        img=G(z,lbl)*0.5+0.5\n",
    "        save_image(img, sample_dir/f'{i}.png')\n",
    "len(list(sample_dir.iterdir()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93de0789",
   "metadata": {},
   "source": [
    "## 2. FID vs Real MNIST (subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2b65c500",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\acm\\models\\jul25\\nullclass\\.venv\\Lib\\site-packages\\torch_fidelity\\datasets.py:16: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  img = torch.ByteTensor(torch.ByteStorage.from_buffer(img.tobytes())).view(height, width, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'frechet_inception_distance': 60.570133823208806}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchvision import datasets, transforms\n",
    "real_dir=Path('task5_real'); real_dir.mkdir(exist_ok=True)\n",
    "if not any(real_dir.iterdir()):\n",
    "    tf=transforms.Compose([transforms.ToTensor(), transforms.Normalize([0.5],[0.5])])\n",
    "    ds=datasets.MNIST(root='./data', train=True, download=True, transform=tf)\n",
    "    for i,(img,_) in enumerate(torch.utils.data.DataLoader(ds,batch_size=1,shuffle=True)):\n",
    "        if i>=500: break\n",
    "        save_image(img*0.5+0.5, real_dir/f'{i}.png')\n",
    "metrics = calculate_metrics(input1=str(sample_dir), input2=str(real_dir), fid=True, kid=False, pr=False, verbose=False, cuda=False)\n",
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5050a71",
   "metadata": {},
   "source": [
    "## 3. Conditional Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "865a27d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': np.float64(0.806),\n",
       " 'precision': 0.8791856357914174,\n",
       " 'recall': 0.806,\n",
       " 'f1': 0.7936060303473031}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf=_build_or_load_mnist_classifier(DEVICE)\n",
    "cond_metrics=conditional_accuracy(G, clf, DEVICE)\n",
    "cond_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f0a627",
   "metadata": {},
   "source": [
    "## 4. Save All Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e6a4694a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fid': 60.570133823208806,\n",
       " 'accuracy': np.float64(0.806),\n",
       " 'precision': 0.8791856357914174,\n",
       " 'recall': 0.806,\n",
       " 'f1': 0.7936060303473031}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_metrics={'fid':metrics.get('frechet_inception_distance'), **cond_metrics}\n",
    "json.dump(all_metrics, open('task5_metrics.json','w'), indent=2); all_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72840e98",
   "metadata": {},
   "source": [
    "## 5. Summary\n",
    "Task 5 metrics computed and stored in task5_metrics.json."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
